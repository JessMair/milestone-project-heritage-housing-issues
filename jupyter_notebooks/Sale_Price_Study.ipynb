{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Sale Price Study**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Answer business requirement 1:\n",
        "* The client is interested in understanding the relationship between the attributes of a house and its market value as a consequence.\n",
        "Therefore, this notebook will demonstrate a correlation study and will display data visualisations in the form of graphs.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/house_price_records.csv \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate code and plots/graphs that answers business requirement 1 and can be used to build the Streamlit App \n",
        "\n",
        "## Additional \n",
        "\n",
        "* \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'outputs/datasets/collection/house_prices_records.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39moutputs/datasets/collection/house_prices_records.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      3\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/datasets/collection/house_prices_records.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/house_prices_records.csv\"))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now familiarise ourselves with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas_profiling\u001b[39;00m \u001b[39mimport\u001b[39;00m ProfileReport\n\u001b[0;32m----> 2\u001b[0m pandas_report \u001b[39m=\u001b[39m ProfileReport(df\u001b[39m=\u001b[39mdf, minimal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m pandas_report\u001b[39m.\u001b[39mto_notebook_iframe()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=df, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation study"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will explore the missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df[df.columns[df.isna().sum() > 0]]\n",
        "vars_with_missing_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will impute the missing values in the data with the most common value for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "catagorical_missing_var = (vars_with_missing_data\n",
        "                    .columns[vars_with_missing_data.dtypes == 'object']\n",
        "                    .to_list())\n",
        "catagorical_missing_var"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use catagorical Imputer to fill in the missing values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "categorical_imputer = CategoricalImputer(imputation_method='frequent',\n",
        "                                         variables=catagorical_missing_var)\n",
        "df = categorical_imputer.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.filter(catagorical_missing_var).info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now use OneHotEncoder to complete the transformation for the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.encoding import OneHotEncoder\n",
        "encoder = OneHotEncoder(variables=df.columns[df.dtypes=='object'].to_list(), drop_last=False)\n",
        "df_ohe = encoder.fit_transform(df)\n",
        "print(df_ohe.shape)\n",
        "df_ohe.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use .corr() for spearman and pearson and investigate the top 10 correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_spearman = df_ohe.corr(method='spearman')['SalePrice'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
        "corr_spearman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_pearson = df_ohe.corr(method='pearson')['SalePrice'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
        "corr_pearson"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above figures show strong correlations between the variables and the 'SalePrice'.\n",
        "We will look further at the top correlations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_n = 5\n",
        "set(corr_pearson[:top_n].index.to_list() + corr_spearman[:top_n].index.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_to_study = ['1stFlrSF', 'GarageArea', 'GrLivArea', 'OverallQual', 'TotalBsmtSF', 'YearBuilt']\n",
        "vars_to_study"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA on selected variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eda = df.filter(vars_to_study + ['SalePrice'])\n",
        "df_eda.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Visualisation "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will map plots below to gain insights regarding the high scoring variables and their relationship to the 'SalePrice' and save the outputs.\n",
        "We will use feature_engine discretiser and in particular the equal frequancy method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "discretiser = EqualFrequencyDiscretiser(q=6, variables=['SalePrice'])\n",
        "discretiser.fit(df_eda)\n",
        "df_eda = discretiser.transform(df_eda)\n",
        "df_eda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "discretiser.binner_dict_['SalePrice']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now create a map to replace the 'SalePrice' variable with more informative levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = discretiser.binner_dict_['SalePrice']\n",
        "n_factor = len(labels)-1\n",
        "labels_map = {}\n",
        "\n",
        "for n in range(0, n_factor):\n",
        "    if n == 0:\n",
        "        labels_map[n] = f\"< {labels[1]}\"\n",
        "    elif n < n_factor - 1:\n",
        "        labels_map[n] = f\"+{labels[n]}\"\n",
        "    else:\n",
        "        labels_map[n] = f\"{labels[n]} to - {labels[n+1]}\"\n",
        "\n",
        "labels_map"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the 'SalePrice' ranges and thus have 'bins' which the properties can be allocated to, as per their sale price.\n",
        "Lets view these in a dataFrame format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eda['SalePrice'] = df_eda['SalePrice'].replace(labels_map)\n",
        "df_eda"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will plot histograms to visualise the above data. This will make it easier to digest the above data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hue_order = labels_map.values()\n",
        "list(hue_order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "def plot_numerical(df, col, target_var, hue_order):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(data=df, x=col, hue=target_var, hue_order=hue_order, kde=True,\n",
        "                 element=\"step\")\n",
        "    plt.title(f\"{target_var} distribution\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "target_var = 'SalePrice'\n",
        "for col in ['1stFlrSF', 'GarageArea', 'GrLivArea', 'OverallQual', 'TotalBsmtSF', 'YearBuilt']:\n",
        "    plot_numerical(df_eda, col, target_var, hue_order)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will delve further by mapping the variables of our hypothesis againstthe sale price individually. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pearson = df.corr(method='pearson')['SalePrice'].filter(['GrLivArea'])\n",
        "df_pearson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = 'GrLivArea', 'SalePrice'\n",
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "sns.scatterplot(data=df, x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pearson correlation score points towards the fact that the size of the house has a relatively high correlation to the value of the house. \n",
        "The scatter plot and histogram we mapped earlier, both illustrate that as the property size increases, so does the sale price. This supports our first hypothesis. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will explore the second third hypothesis regarding house age against the sale price. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pearson = df.corr(method='pearson')['SalePrice'].filter(['YearBuilt'])\n",
        "df_pearson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = 'YearBuilt', 'SalePrice'\n",
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "sns.scatterplot(data=df, x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the correlation does exist, but is not as strong or clear. For example, the most expensive property may be the most recent, however there are instances where a property built in 1920 can be valued the same as a property built in more recent times."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will explore the third hypothesis by examing the overall quality of the property against the sale price. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pearson = df.corr(method='pearson')['SalePrice'].filter(['OverallQual'])\n",
        "df_pearson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = 'OverallQual', 'SalePrice'\n",
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "sns.scatterplot(data=df, x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly indicates that house quality is key to  increasing the house price and the correlation score is also very high. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although the year a property may be remodelled is not one of the top corraltive indicators of sale price, it would be good to explore the data to see if more recent refurbishments will have a positive imapct on the sale price. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pearson = df.corr(method='pearson')['SalePrice'].filter(['YearRemodAdd'])\n",
        "df_pearson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = 'YearRemodAdd', 'SalePrice'\n",
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "sns.scatterplot(data=df, x=x, y=y)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although the imapct is significant, it does not appear to have an overwhelming impact. If we consider that a property remodelled in 1950 is valued higher than some properties remodelled in the most recent years, this shows that the other factors must be coming in to play. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_numerical(df, col, target_var, hue_order):\n",
        "#     plt.figure(figsize=(8, 5))\n",
        "#     sns.histplot(data=df, x=col, hue=target_var, hue_order=hue_order, kde=True,\n",
        "#                  element=\"step\")\n",
        "#     plt.title(f\"{target_var} \", fontsize=20, y=1.05)\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# target_var = 'OverallQual'\n",
        "# for col in ['YearBuilt']:\n",
        "#     plot_numerical(df_eda, col, target_var, hue_order)\n",
        "#     print(\"\\n\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* House lot size has a big impact on house price. The histogram plots show that where the 'GrLivArea', '1stflrSF', 'GarageArea' and 'TotalBmntSF' have greater square footage the sale price in generally tend to be higher. \n",
        "\n",
        "* The age of the property does also have a relevance to property value, however I would deduce that the quality also has to be present. This I gather from the fact that that most properties from 1920 have a lower sale value which for the most part is ikely due to the overall quality having deterioted. \n",
        "A new property is likely to be of a higher quality, therefore this can be a reason why there tends to be more instances of a higher sale price. Although the data does not conclude that these factors alone can increase the value. It requires supporting factors, such as size for example. \n",
        "We have explored this by plotting the 'YearRemodadd' variable, and the data clearly indicates having remodelled a house recently will not necessairly mean it will outperform other deducing factors. This may be because the remodels have not all been to a high standard, but again this would be an assumption. There is not enough data regarding the remodelling to conclude its effects. Having data that showed the sale price before and after a remodelling would provide a clearer insight into its positive effects. \n",
        "\n",
        "* It is clear that no variable alone is powerful enough to dictate the highest sale price, but from the above observations, one can conclude that all other things being equal, the higher the overall quality, the higher the sale price will be. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
